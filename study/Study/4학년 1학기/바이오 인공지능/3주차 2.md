- 선형회귀
	- Linear Regression
		- yi= w1x1 + w2x2 + w3x3 + ... + wixi + ei(입실론값) 
		- yi = y^hat + ei(입실론)
		- ei = yi - y^hat (실제값 - 예측값 = error를 의미)
	- Loss
		- LSE (Least square error)
			- loss = 1 / n sigma ei = 1 / n sigma (yi - y^hati)^2
	
	- 최적화
		- 정규방정식을 활용하여 오차가 최소화 되는 지점, 미분 후 0이 되는 지점을 직접 계산함
		- e = y - y^hat = y - x \* theta
		- SSE = e^t \* e -> 미분 -> theta 구하기
	
		- X^T * X 의 계산이 어려울 경우
			- 경사하강법 또는 확률적 경사하강법을 사용

			- 가중치 업데이트
				- W = W - alpha * thetaMSE / thetaW
			- 절편 업데이트
				- b = b - alpha * thetaMSE / thetab

- 로지스틱 회귀
	- 