- 선형회귀
	- Linear Regression (연속형 값, 수치형)
		- yi= w1x1 + w2x2 + w3x3 + ... + wixi + ei(입실론값) 
		- yi = y^hat + ei(입실론)
		- ei = yi - y^hat (실제값 - 예측값 = error를 의미)
	- Loss
		- LSE (Least square error)
			- loss = 1 / n sigma ei = 1 / n sigma (yi - y^hati)^2
	
	- 최적화
		- 정규방정식을 활용하여 오차가 최소화 되는 지점, 미분 후 0이 되는 지점을 직접 계산함
		- e = y - y^hat = y - x \* theta
		- SSE = e^t \* e -> 미분 -> theta 구하기
	
		- X^T * X 의 계산이 어려울 경우
			- 경사하강법 또는 확률적 경사하강법을 사용

			- 가중치 업데이트
				- W = W - alpha * thetaMSE / thetaW
			- 절편 업데이트
				- b = b - alpha * thetaMSE / thetab

- 로지스틱 회귀 (범주형 값, class)
	- 이진 분류일 때 많이 사용, 다중 분류에도 사용
	- P(Y = 1 | X) = 1 / (1 + e^(w1x1 + w2x2 + w3x3 + ... + b0))
	
	- odds
		- 어떤 사건이 발생할 확률과 발생하지 않을 확률의 비율
		- 성공확률 / 실패확률
		- P(Y = 1) / (1 - P(Y = 1))

		- 베르누이 분포로 나타내면
			- p / (1 - p)
	- logit
		- ln (logit) = ln (p / (1 - p))
	
	- logistic 회귀
		- 해당 logit을 선형 함수의 형태로 모델링 할 수 있을 것이라고 가정
		- f(x) = b0 +b1x1 +b2x2 + ... + bnxn
		- ln(p / (1 - p)) = f(x)
		- e^f(x) = p / (1-p)
		- 1 + e^f(x) = 1 + p / (1 - p) = (1 - p + p) / (1 - p) = 1 / (1 - p)
		- (1 + e^f(x)) * p = p / (1 - p) -> (1+e^f(x)) * p =  e^f(x)
		- p = e^f(x) / (1 + e^f(x)) = 1 / (1 + e^-f(x))
	
	- logistic 회귀의 최적화
		- 최대 우도 추정으로 최적화
			- ppt 15page 참고
			- CE와 비슷함
	
	- 다중 로지스틱 회귀
		- P(Y=1) ,P(Y=2), P(Y=3), ... 에 대해 모두 구함
		- 이후 일반화 P(Y=K)에 대해 식을 세움

- 서포트 벡터 머신 (SVM)
	- 분류 및 회귀 분석에 사용되는 지도학습 알고리즘
	-  결정 경계를 학습하여 데이터를 분류함
		- 초평면을 찾고, 초평면과 가장 가까운 데이터 포인트(서포트 벡터)사이의 마진을 최대화 하는것이 목표
	- 종류
		- 선형 분류 SVM
			- 데이터가 선형적으로 구분 가능한 경우
		- 비선형 분류 SVM
			- 커널 트릭
				- 데이터를 고차원으로 변환하지 않고도 내적연산을 이용해 고차원에서 계산한 것처럼 처리하는 기법
				- 원래 데이터에 z값을 부여, 마치 고차원에서 계산한 것처럼 처리
	
	- 파라미터
		- C
			- SVM의 Regularization(규제) 강도를 조절하는 하이퍼 파라미터
			- Regularization
				- low일 수록 단일 직선에 가까워짐
				- high일 수록 다양한 직선이 생김
		- gamma
			- SVM의 RBF(Radial Basis Function) 커널에서 데이터 샘플 간 거리를 측정하는데 사용되는 하이퍼 파라미터
				- low
					-  먼 data도 참고됨
				- high
					- 근처 data만 참고됨
- 트리 기반 분류
	- 결정 트리(Decision Tree)
		- 각 노드는 특정한 특성에 대한 조건을 기반으로 데이터를 분할
		
		- 지니 인덱스 (GINI Index)
			- 1 - sigma pj^2 
			- 최솟값
				- 모든 데이터가 하나의 class이므로 0으로 나옴 
			- 나뉘어진 노드의 데이터 비율에 맞춰 지니 인덱스의 합을 구함
				- 지니 인덱스합이 작을수록 구분을 잘했다는 의미
			
		- 장점
			- 수치형, 범주형 데이터에 모두사용 가능
			- 간단하고 해석이 쉬움
			- 전처리가 적음
				- 자기 자신의 데이터에 대해서 분류함
		- 단점
			- 과적합 가능
			- 작은 변화에도 민감
	
	- 랜덤 포레스트
		- 여러개의 decision tree를 조합하는 앙상블 학습(Ensemble Learning)기법
		- 다수의 결정 트리 모델을 만들고 투표 또는 평균을 통해 최종 결과를 예측

		- 부트스트랩 샘플링 (Bootstrap Sampling)
			- 원본 데이터에서 랜더마게 일부 데이터를 샘플링 
			- 여러개의 학습용 데이터를 생성
		- 특성 무작위 선택 (Random Feature Selection)
			- 각 트리를 만들 때, 랜덤하게 일부 특성만 선택하여 학습
				- 개별 트리의 상관성을 낮춤
	
	- XGBoost
		- Gradient Boosting을 최적화한 강력한 머신러닝 알고리즘
			- 하나의 데이터 -> 트리, weight -> weight 업데이트
			- 여러 트리의 결과를 합하여 만듦
		- <사실상 제일 좋음>
		- sklearn에는 없으나, 문법은 동일
		- 