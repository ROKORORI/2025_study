- 선형회귀
	- Linear Regression (연속형 값, 수치형)
		- yi= w1x1 + w2x2 + w3x3 + ... + wixi + ei(입실론값) 
		- yi = y^hat + ei(입실론)
		- ei = yi - y^hat (실제값 - 예측값 = error를 의미)
	- Loss
		- LSE (Least square error)
			- loss = 1 / n sigma ei = 1 / n sigma (yi - y^hati)^2
	
	- 최적화
		- 정규방정식을 활용하여 오차가 최소화 되는 지점, 미분 후 0이 되는 지점을 직접 계산함
		- e = y - y^hat = y - x \* theta
		- SSE = e^t \* e -> 미분 -> theta 구하기
	
		- X^T * X 의 계산이 어려울 경우
			- 경사하강법 또는 확률적 경사하강법을 사용

			- 가중치 업데이트
				- W = W - alpha * thetaMSE / thetaW
			- 절편 업데이트
				- b = b - alpha * thetaMSE / thetab

- 로지스틱 회귀 (범주형 값, class)
	- 이진 분류일 때 많이 사용, 다중 분류에도 사용
	- P(Y = 1 | X) = 1 / (1 + e^(w1x1 + w2x2 + w3x3 + ... + b0))
	
	- odds
		- 어떤 사건이 발생할 확률과 발생하지 않을 확률의 비율
		- 성공확률 / 실패확률
		- P(Y = 1) / (1 - P(Y = 1))

		- 베르누이 분포로 나타내면
			- p / (1 - p)
	- logit
		- ln (logit) = ln (p / (1 - p))
	
	- logistic 회귀
		- 해당 logit을 선형 함수의 형태로 모델링 할 수 있을 것이라고 가정
		- f(x) = b0 +b1x1 +b2x2 + ... + bnxn
		- ln(p / (1 - p)) = f(x)
		- e^f(x) = p / (1-p)
		- 1 + e^f(x) = 1 + p / (1 - p) = (1 - p + p) / (1 - p) = 1 / (1 - p)
		- (1 + e^f(x)) * p = p / (1 - p) -> (1+e^f(x)) * p =  e^f(x)
		- p = e^f(x) / (1 + e^f(x)) = 1 / (1 + e^-f(x))
	
	- logistic 회귀의 최적화
		- 최대 우도 추정으로 최적화
			- ppt 15page 참고
			- CE와 비슷함
	
	- 다중 로지스틱 회귀
		- P(Y=1) ,P(Y=2), P(Y=3), ... 에 대해 모두 구함
		- 이후 일반화 P(Y=K)에 대해 식을 세움

- 서포트 벡터 머신 (SVM)
	- 분