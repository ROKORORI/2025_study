- 선형회귀
	- Linear Regression (연속형 값, 수치형)
		- yi= w1x1 + w2x2 + w3x3 + ... + wixi + ei(입실론값) 
		- yi = y^hat + ei(입실론)
		- ei = yi - y^hat (실제값 - 예측값 = error를 의미)
	- Loss
		- LSE (Least square error)
			- loss = 1 / n sigma ei = 1 / n sigma (yi - y^hati)^2
	
	- 최적화
		- 정규방정식을 활용하여 오차가 최소화 되는 지점, 미분 후 0이 되는 지점을 직접 계산함
		- e = y - y^hat = y - x \* theta
		- SSE = e^t \* e -> 미분 -> theta 구하기
	
		- X^T * X 의 계산이 어려울 경우
			- 경사하강법 또는 확률적 경사하강법을 사용

			- 가중치 업데이트
				- W = W - alpha * thetaMSE / thetaW
			- 절편 업데이트
				- b = b - alpha * thetaMSE / thetab

- 로지스틱 회귀 (범주형 값, class)
	- 이진 분류일 때 많이 사용, 다중 분류에도 사용
	- P(Y = 1 | X) = 1 / (1 + e^(w1x1 + w2x2 + w3x3 + ... + b0))
	
	- odds
		- 어떤 사건이 발생할 확률과 발생하지 않을 확률의 비율
		- 성공확률 / 실패확률
		- P(Y = 1) / (1 - P(Y = 1))

		- 베르누이 분포로 나타내면
			- p / (1 - p)
	- logit
		- ln (logit) = ln (p / (1 - p))
	
	- logistic 회귀
		- 해당 logit을 선형 함수의 형태로 모델링 할 수 있을 것이라고 가정
		- f(x) = b0 +b1x1 +b2x2 + ... + bnxn
		- ln(p / (1 - p)) = f(x)
		- e^f(x) = p / (1-p)
		- 1 + e^f(x) = 1 + p / (1 - p) = (1 - p + p) / (1 - p) = 1 / (1 - p)
		- (1 + e^f(x)) * p = p / (1 - p) -> (1+e^f(x)) * p =  e^f(x)
		- p = e^f(x) / (1 + e^f(x)) = 1 / (1 + e^-f(x))
	
	- logistic 회귀의 최적화
		- 최대 우도 추정으로 최적화
			- ppt 15page 참고
			- CE와 비슷함
	
	- 다중 로지스틱 회귀
		- P(Y=1) ,P(Y=2), P(Y=3), ... 에 대해 모두 구함
		- 이후 일반화 P(Y=K)에 대해 식을 세움

- 서포트 벡터 머신 (SVM)
	- 분류 및 회귀 분석에 사용되는 지도학습 알고리즘
	-  결정 경계를 학습하여 데이터를 분류함
		- 초평면을 찾고, 초평면과 가장 가까운 데이터 포인트(서포트 벡터)사이의 마진을 최대화 하는것이 목표
	- 종류
		- 선형 분류 SVM
			- 데이터가 선형적으로 구분 가능한 경우
		- 비선형 분류 SVM
			- 커널 트릭
				- 데이터를 고차원으로 변환하지 않고도 내적연산을 이용해 고차원에서 계산한 것처럼 처리하는 기법
				- 원래 데이터에 z값을 부여, 마치 고차원에서 계산한 것처럼 처리
	
	- 파라미터
		- C
			- SVM의 Regularization(규제) 강도를 조절하는 하이퍼 파라미터
			- Regularization
				- low일 수록 단일 직선에 가까워짐
				- high일 수록 다양한 직선이 생김
		- gamma
			- SVM의 RBF(Radial Basis Function) 커널에서 데이터 샘플 간 거리를 측정하는데 사용되는 하이퍼 파라미터
				- low
					-  먼 data도 참고됨
				- high
					- 근처 data만 참고됨