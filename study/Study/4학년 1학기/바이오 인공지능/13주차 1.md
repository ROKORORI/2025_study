- 사전학습 모델
	- 대규모 데이터(주로 라벨이 없는)에서 일반적인 데이터의 패턴을 학습한 후, 다양한 직업에 활용되는 모델
	- 학습구조
		- Pretraining
			- 일반 데이터에서 보편적인 패턴 학습
		- Fine-tuning
			- 특정 문제에 맞춰 조정
	- 생물정보학에서도 전사체, 염기서열 분석등에 적용되고 있음

- Pretraining과 Fine-tuning의 구조
	- Pretraining
		- 라벨이 없는 대규모 데이터로 사전학습
		- 자기지도학습(self-supervised learning)방식 사용
	- Fine tuning
		- 소량의 라벨된 데이터로 task-specific 조정
		- 기존 지식을 재활용하여 빠르고 효율적인 학습 가능
		- 데이터가 적어도 성능 확보, 학습 시간 단축
	- 대표 학습 방식
		- Masked Language Modeling(MLM)
			- 입력에서 일부토큰을 mask 처리하고 이를 예측
		- Next Token/Setence Prediction
			- 다음 토큰, 문장을 예측
		- Contrastive Learning
			- 비슷한 샘플은 임베딩 공간에서 가깝게, 다른 샘플은 멀게 학습

- 생물정보학과 사전학습의 궁합
	- 생물학적 데이터의 특징
		- 고차원
		- 희소
		- noisy
		- batch effect 존재
			- 과적합이 자주 발생
	- 사전학습이 특히 유용한 이유
		- 다양한 데이터에서 일반적이면서 공통 표현 학습 가능
		- 적은 수의 샘플에도 일반화 성능 확보 가능
	- 즉, 의미있는 임베딩 공간을 학습
		- 유전자 서열의 유사도
		- 전사체 유전자 발현량 패턴의 유사도
		- 병리학적 패턴의 유사도
		
		- 유사 하다는 의미
			- 생물학적 기능, 현상이 유사할 가능성 증가

- 사전학습 모델의 전이 학습 효과
	- 사전학습 모델은 하나의 task에만 적용되는 것이 아니라, 여러 downstream task에 쉽게 전이되어 활용될 수 있음
	- ex)
		- DNABERT
			- DNA 염기서열 학습
			- 유전자 기능 예측, enhancer, promoter 분류
			- 스플라이싱 부위 예측등 다양한 작업에 활용 가능
		- Geneformer
			- 전사체 데이터를 기반
			- 유전자 간 관계 학습, 환자 하위군 분류, 약물 반응 예측에 사용됨
	- Pretrained 모델의 핵심은 하나의 표현 공간을 다양한 생물학적 문제에 재사용하는 능력

- 자연어와 유전체의 구조적 유사성
	- 자연어와 DNA 염기서열은 구조적으로 유사한 점이 많음, 이 때문에 NLP에서 사용된 모델들이 유전체 분석에 자연스럽게 적용될 수 있음
	
	- 비교대상
		- 문장 ->
			- 염기 서열
		- 단어 ->
			- k-mer, 염기 1개
	- 이러한 유사성 덕분에 BERT 구조를 그대로 DNA에 적용한 DNABERT 같은 모델이 성능을 보일 수 있음

- 전사체 데이터의 사전학습 모델 입력의 형태
	- 전사체 데이터는 각 유전자에 대한 발현값을 수치형으로 표현한 벡터로 활용
		- 이러한 벡터는 자연어 문장에서 단어들이 모여 문맥을 이루는 것과 유사하게, 유전자들의 조합과 상관성(co-expression)속에 의미가 존재함
		- 다만, 문장처럼 인접한 단어 라는 개념이 희박하기에 효율적인 문맥 인코딩(context encoding; position encoding)이 필요함