- 사전학습 모델
	- 대규모 데이터(주로 라벨이 없는)에서 일반적인 데이터의 패턴을 학습한 후, 다양한 직업에 활용되는 모델
	- 학습구조
		- Pretraining
			- 일반 데이터에서 보편적인 패턴 학습
		- Fine-tuning
			- 특정 문제에 맞춰 조정
	- 생물정보학에서도 전사체, 염기서열 분석등에 적용되고 있음

- Pretraining과 Fine-tuning의 구조
	- Pretraining
		- 라벨이 없는 대규모 데이터로 사전학습
		- 자기지도학습(self-supervised learning)방식 사용
	- Fine tuning
		- 소량의 라벨된 데이터로 task-specific 조정
		- 기존 지식을 재활용하여 빠르고 효율적인 학습 가능
		- 데이터가 적어도 성능 확보, 학습 시간 단축
	- 대표 학습 방식
		- Masked Language Modeling(MLM)
			- 입력에서 일부토큰을 mask 처리하고 이를 예측
		- Next Token/Setence Prediction
			- 다음 토큰, 문장을 예측
		- Contrastive Learning
			- 비슷한 샘플은 임베딩 공간에서 가깝게, 다른 샘플은 멀게 학습

- 생물정보학과 사전학습의 궁합
	- 생물학적 데이터의 특징
		- 고차원
		- 희소
		- noisy
		- batch effect 존재
			- 과적합이 자주 발생
	- 사전학습이 특히 유용한 이유
		- 다양한 데이터에서 일반적이면서 공통 표현 학습 가능
		- 적은 수의 샘플에도 일반화 성능 확보 가능
	- 즉, 의미있는 임베딩 공간을 학습
		- 유전자 서열의 유사도
		- 전사체 유전자 발현량 패턴의 유사도
		- 병리학적 패턴의 유사도
		
		- 유사 하다는 의미
			- 생물학적 기능, 현상이 유사할 가능성 증가

- 사전학습 모델의 전이 학습 효과
	- 사전학습 모델은 하나의 task에만 적용되는 것이 아니라, 여러 downstream task에 쉽게 전이되어 활용될 수 있음
	- ex)
		- DNABERT
			- DNA 염기서열 학습
			- 유전자 기능 예측, enhancer, promoter 분류
			- 스플라이싱 부위 예측등 다양한 작업에 활용 가능
		- Geneformer
			- 전사체 데이터를 기반
			- 유전자 간 관계 학습, 환자 하위군 분류, 약물 반응 예측에 사용됨
	- Pretrained 모델의 핵심은 하나의 표현 공간을 다양한 생물학적 문제에 재사용하는 능력

- 자연어와 유전체의 구조적 유사성
	- 자연어와 DNA 염기서열은 구조적으로 유사한 점이 많음, 이 때문에 NLP에서 사용된 모델들이 유전체 분석에 자연스럽게 적용될 수 있음
	
	- 비교대상
		- 문장 ->
			- 염기 서열
		- 단어 ->
			- k-mer, 염기 1개
	- 이러한 유사성 덕분에 BERT 구조를 그대로 DNA에 적용한 DNABERT 같은 모델이 성능을 보일 수 있음

- 전사체 데이터의 사전학습 모델 입력의 형태
	- 전사체 데이터는 각 유전자에 대한 발현값을 수치형으로 표현한 벡터로 활용
		- 이러한 벡터는 자연어 문장에서 단어들이 모여 문맥을 이루는 것과 유사하게, 유전자들의 조합과 상관성(co-expression)속에 의미가 존재함
		- 다만, 문장처럼 인접한 단어 라는 개념이 희박하기에 효율적인 문맥 인코딩(context encoding; position encoding)이 필요함
		- 벡터상 인접 != 유전자 기능상 유사

- 병리이미지 데이터의 사전학습 모델 입력 형태
	- 병리 이미지 사전학습에서는 WSI를 tile 단위로 나눈 후, 다양한 augmentation기법을 통해 tile간 다양성과 표현 학습 능력을 강화함
	
	- 색상 기반 변형
		- Color jitter
			- 밝기, 대비, 채도, 색조 무작위 조절
		- H & E stain augmentation
			- Hematoxylin & Eosin 염색의 농도 차이 시물레이션
	- 조직 구조 보존 강화
		- Gaussian blur
			- 저해상도 시물레이션
	- 배경 제거, 중심 타겟팅
		- Tissue detection 후 foreground tile만 사용

- 사전학습 모델의 응용 방식
	- Fine tuning
		- 단백질 기능 예측
			- 사전학습 단계
				- ProtBERT, ESM 등의 모델은 수천만개의 단백질 서열을 self-supervised 방식으로 학습 (MLM으로 서열 내 일부 아미노산을 가리고 예측)
			- Fine tuning 단계
				- 학습된 모델의 임베딩을 활용하여 Go term 분류기 또는 multi label classifier를 훈련
				- task-specific labeled dataset(Swiss-Prot annotations)사용
	- Transfer Learning
		- 세포주
			- 배양한 세포
		- 세포주 -> 환자 데이터
			- 목표
				- 세포주 수준의 약물 반응성 예측을 환자 데이터 상의 약물 반응성 예측으로 전이
					- 환자 수준의 데이터에서 약물 반응성 데이터 수집은 매우 힘듦
			- 전이학습이 필요한 이유
				- 세포주 데이터의 분포와 환자 데이터의 분포가 상이하기에 분포를 맞춰주는 과정이 필수적임
	- Few-shot Learning
		- 대부분의 희귀 유전 질환은 환자 수가 적어 충분한 학습 데이터 확보가 어려움
		- 진단되지 않은 환자는 표현형(phenotype)정보만 존재하고 정답 유전자 정보는 없음
			- 새로운 질환이나 매우 적은 샘플을 다루기 위해 Few shot learning이 필요함
		- SHEPHERD(Scalable and interpretable few-shot phenotype-based rare disease diagnosis)
			- Biomedical Knowledge Graph(KG)기반 표현 학습
			- 환자 표현형 (HPO terms)와 유전자 기능 정보를 연결
			- Few-shot setting에서도 새로운 질환에 대한 진단 가능
			
			- 주요 구성 요소
				- 지식 그래프 임베딩(ComplEx)
					- 유전자, 질환, 표현형의 의미적 관계학습
				- 환자 표현형 벡터와
					- HPO term set을 그래프에서 인코딩
				- Few shot inference
					- 단일 환자 표현형만으로 유력 인과 유전자 순위화
	- Zero-shot Learning
		- 약물 재창출(TxGNN)
			- 목표
				- 치료제가 없는 희귀질환의 약물 재창출
					- 대부분의 희귀 질환을 승인된 치료제가 없으며, 분자 수준의 정보도 부족함
					- 기존의 AI기반 약물 재창출 모델은 이미 알려진 질환에만 적용 가능
						- Zero shot learning을 통해 치료제가 전무한 질환에 대한 약물 후보 예측 필요
			-  TxGNN(Therapeutics Graph Neural Network)
				- 의료 지식 그래프를 기반으로 한 그래프 신경망(GNN)모델
				- Metric Learning을 활용하여 치료제가 없는 질환에 대한 약물 후보 예측 수행
				- Explainer 모듈을 통해 예측결과에 대한 해석 가능성 제공