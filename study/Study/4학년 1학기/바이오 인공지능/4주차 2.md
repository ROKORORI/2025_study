- 퍼셉트론(인공신경망)
	- 요소
		- w, x, t, o, gamma
			-  가중치, 입력, 출력, 실제 출력, 학습률
	- 비선형 activation function이 필요
		- 선형일경우, 아무리 많이 쌓아도 한층인것과 같음.
	- 강하학습법
		- 미분해가며 update
	- problem
		- 실제값과 예측값간 차이로 학습이 필요
		- sol
			- 역전파학습
	- 역전파
		- chain rule을 이용해 은닉층까지 거꾸로 전파하면서, 각층의 가중치를 조정할 수 있도록 함

- CNN
	- 이미지
		- 유클리디안 공간에 존재 
	- 이미지, 시각적 처리에 특화된 DNN
		- filter 1개가 여러 이미지를 처리할 수 있어 efficient함
	
	- 구성
		- convolution layer
			- feature extraction
		- pooling layer
			- 데이터 축소
		- fully connected layer
			- classification
	
	- convolution
		- filter에 따라 convolution 연산 이후, image의 size가 줄기도함
		- solution
			- padding
				- 주위에 0 또는 1을 두름
	
	- pooling
		- feature map을 줄이거나 강조할때 사용
			- 보통 pooling size와 스트라이드를 동일하게 설정함
		- 방법
			- max
				- 하나의 특징
			- average
				- 전체적인 특징

- RNN
	- 유클리디안 공간에 존재
	- 시계열 데이터, 순서가 중요한 데이터일때 사용
		- 요즘엔 transformer 구조를 대부분 사용
	
	- 문제점
		- 장기 의존성 문제
			- 오래된 정보는 잊혀지기 쉬움
				- lstm, gru
		- 계산이 느리고 병렬화 어려움
			- transformer
	
	- LSTM
		- 기억 셀과 게이트 구조를 사용해 무엇을 기억하고, 무엇을 잊을지 스스로 결정함
		- gate
			- input 
			- forget 
				- 이전 정보를 어느정도 기억할 것인가 결정
			- output
		- state
			- hidden
			- cell
	- GRU
		- LSTM보다 간단하고 계산 효율이 좋음
		- forget gate와 input gate를 하나의 update gate로 합침
		- cell state와 hidden state가 합쳐짐
- GNN (Graph Neural Network)
	- idea
		- 그래프 상에 연결된 주변 node의 정보를 수집하여, 자신의 정보를 업데이트
	- GCN(Graph Convolution Network)
		- 장점
			- 그래프 구조에 효율적
			- permutation에 무관
			- O(E)로 edge개수에 비례한 연산량을 가짐
			- Transductive
				- 기존 그래프 학습
			- Inductive 
				- 신규 그래프 학습
			- 두 학습방법 모두 사용 가능
		- 단점
			- 큰 그래프에 비효율적임
			- 그래프 동형성에 취약
				- 구조적으로 다른데, 같은 결과가 나오기도 함 (그래프 구조가 비슷할 경우)
				- 모든 주변 노드를 동일하게 취급
		
		- GraphSAGE
			- 큰 그래프 취약점 극복하고자 함
		- GIN
			- Neighbor 정보 받는 방법을 바꾸면, 그래프 동형성 취약을 극복할 수 있음
		- GAT
			- 주변 노드중 attention을 통해 영향이 큰 노드를 반영
	- 사용
		- node classification
		- graph classification
		- link prediction