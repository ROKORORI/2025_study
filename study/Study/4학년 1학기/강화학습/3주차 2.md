- 복습
	- MDP
		- 강화 학습의 과정을 표현하기 위한 방법
	- 오류 수정
		- 3,2 -> 4,3 = reward = 1
		- 4,3 -> 3,2 = reward= -1
		- reward에 확률을 안곱해도됨
		- 곱하면 expected reward

- MDP
	-  주요파라미터
		- (S, A, P, R, r(gamma))
		- state, action, state transition Probability, Reward , gamma(조정)
	- gamma
		- 1 = 미래 보상 가중치
		- 0 = 현재에 집중

- Reward
	- 정의
		- scalar Feedback
	
	- agent의 goal
		- maximize the cumulative sum of rewards임
	
	- Reward Hypothesis 
		- 강화학습(Reinforcement Learning)은 Reward Hypothesis에 기준을 둠 
			- reward의 누적합의 기댓값을 최대화 하는게 agent의 목표임
	
	-  Reward function
		- 다양한 type이 있음
			- R = R(S), R(S, A), R(S, A, S')
		
		- Known dynamics
			- p(s', r | s, a) = (s, a, s, r)
		
		- State Transition Probabilty
			- P^a_ss' = p(s' | s, a) = p(St+1 = s' | St = s, At = a) = sigma p(s',r | s, a)
				- state가 s이고 action을 했을때 s'으로 갈 확률
		
		- Expected Reward for State Action Pair
			- 현재 state에서 reward를 받을 확률
				- s일때 reward를 받을 확률(기댓값)
			- R^a_s = r(s, a) = E\[Rt+1 | St=s, At = a] = sigma r sigma p(s' r | s, a)
		
		- Expected Reward for state action next state triple
			- 다음 state에서 reward 를 받을 확률
				- s'일때 reward를 받을 확률(기댓값)
			- R^a_ss' = r(s, a, s') = E\[Rt+1 | St = s, At = a, St+1 = s']
			- = sigma r \* p(s', r | s, a) / p(s' | s, a)
		
		- 예시
			- 1)
			- s1 -> a0일때, expected reward
			- 0.7 * 5 + 0.1 * 0 + 0.2 = 3.5
			
			- 2)
			- S2, a1action, next state = a1, next state = s0일때 확률
	
-  Return
	- Gt
		- episode의 총 reward = summation of reward
		- sigma k = (0 ~ inf) gamma^k \* Rt+k+1
	
	- Discount value r(Gamma)
		- 0 ~ 1 범위를 가짐
			- 보통 0.9정도로 설정
		- 1 =  미래 보상에 대한 가중치
		- 0 = 현재에 집중
		- 현재 reward와 미래 reward를 구분하기 위해 사용
	
	- MDP에서 Discounted value를 왜 사용하는가? 
		- Return이 무한이 안되도록함
		- 미래의 불확실성
		- 현실에선 종종 현재 보상이 미래의 보상보다 중요하기도 함

- Policy
	- state -> 확률을 내보냄
	
	- stochastic policy
		- PI = pi(a | s) = P(at = a | st = s)
		- 확률론적 policy
			- MDP에서 사용
	- deterministic policy
		- PI = pi(s) = a
		- 결정론적 policy
	
	- known MDP (벨만)
		- 모든 상황을 알 수 있다.
		- deterministic policy
		- optimal value를 찾을 수 있음
	- unknown MDP (강화학습의 기초)
		- 모든 상황을 알 수 없음.
		- e-greedy policy
			- exploration 하기 위함
			- stochastic policy
			- 1-앱실론
				- optimal value
			- 앱실론 (5 ~ 10% 작은값)
				- randomly

- notation
	- max f(a)
		- f(a)의 최댓값
	- argmaxa f(a)
		- f(a)가 가장크게 만들 수 있는 a값
	- pi
		- policy
	- pi(a | s)
		- stochastic policy
	- pi(s)
		- deterministic policy
	- v_pi(s)
		- state value function
	- v_*
		- optimal state value function
	- q