- 복습
	- MDP
		- 강화 학습의 과정을 표현하기 위한 방법
	- 오류 수정
		- 3,2 -> 4,3 = reward = 1
		- 4,3 -> 3,2 = reward= -1
		- reward에 확률을 안곱해도됨
		- 곱하면 expected reward

- MDP
	-  주요파라미터
		- (S, A, P, R, r(gamma))
		- state, action, state transition Probability, Reward , gamma(조정)
	- gamma
		- 1 = 미래 보상 가중치
		- 0 = 현재에 집중

- Reward
	- 정의
		- scalar Feedback
	
	- agent의 goal
		- maximize the cumulative sum of rewards임
	
	- Reward Hypothesis 
		- 강화학습(Reinforcement Learning)은 Reward Hypothesis에 기준을 둠 
			- reward의 누적합의 기댓값을 최대화 하는게 agent의 목표임
	
	-  Reward function
		- 다양한 type이 있음
			- R = R(S), R(S, A), R(S, A, S')
		
		- Known dynamics
			- p(s', r | s, a) = (s, a, s, r)
		
		- State Transition Probabilty
			- P^a_ss' = p(s' | s, a) = p(St+1 = s' | St = s, At = a) = sigma p(s',r | s, a)
				- state가 s이고 action을 했을때 s'으로 갈 확률
		
		- Expected Reward for State Action Pair
			- R^a_s = r(s, a) = E\[Rt+1 | St=s, At = a] = sigma r sigma p(s' r | s, a)
		
		- Expected Reward for state action next state triple
			- R^a_ss' = r(s, a, s') = E\[Rt+1 | St = s, At = a, St+1 = s']
			- = sigma r \* p(s', r | s, a) / p(s' | s, a)