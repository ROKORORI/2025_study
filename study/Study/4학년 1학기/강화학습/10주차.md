- on policy vs off poilicy
	- 학습 대상
		- 현재 실행 정책 vs 다른 정책
	- 탐험 정책
		- 동일 vs 다름
	- 데이터 재사용
		- 어려움 vs 가능
	- 수렴성
		- 느리지만 안정적 vs 빠를 수 있으나 불안정성 있음
	- 대표 알고리즘
		- SARSA, MC Control vs Q-learning, DQN, DDPG, SAC
	- 탐험 제어
		- 직접수행(e-greedy) vs 자유롭게 설계 가능
	- 실시간 적용
		- 좋음 vs 좋지 않음
	- 리플레이 버퍼
		- 사용 어려움 vs 필수 활용 가능

- Importance sampling
	- Sarsa -> Q learning
	- 즉, On policy -> Off policy로 변환하는 효과를 가짐