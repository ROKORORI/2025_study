- on policy vs off poilicy
	- 학습 대상
		- 현재 실행 정책 vs 다른 정책
	- 탐험 정책
		- 동일 vs 다름
	- 데이터 재사용
		- 어려움 vs 가능
	- 수렴성
		- 느리지만 안정적 vs 빠를 수 있으나 불안정성 있음
	- 대표 알고리즘
		- SARSA, MC Control vs Q-learning, DQN, DDPG, SAC
	- 탐험 제어
		- 직접수행(e-greedy) vs 자유롭게 설계 가능
	- 실시간 적용
		- 좋음 vs 좋지 않음
	- 리플레이 버퍼
		- 사용 어려움 vs 필수 활용 가능

- Importance sampling
	- Sarsa -> Q learning
	- 즉, On policy -> Off policy로 변환하는 효과를 가짐

- Bias vs Variance
	- Bias가 높음
		- 편향이 많이 됨
	- Variance가 높음
		- 불안정성이 높음
		- 다른 길로 이동
	- Bias
		- MC
			- unbias 됨
		- TD
			- Estimated Q사용
			- 예측값 이용, 실제값과 격차가 있음
	- Variance
		- MC
			- 여러 step -> 오차 누적 -> Variance가 높음
		- TD
			- 오차가 한번만 벌어짐 1STEP -> Variance가 낮음
	- 정리
		- MC
			- High variance Low bias
		- TD
			- Low variance High bias

- Double Q learning
	- Q table이 2개
	- Q learning의 문제, overestimate
	- 이것을 해결하기 위해 2개의 Q value function 사용
		- Q1 업데이트 -> Q2 사용
		- Q2 업데이트 -> Q1 사용
		- over estimate 문제 완화

- TD(lambda)
	- lambda step 만큼 전개
	- lambda inf
		- terminal까지 가서 update
		- return 값까지 가서 update = MC
		- 일반 TD = 1 STEP
		- 즉, TD와 MC사이 중간 = TD lambda임