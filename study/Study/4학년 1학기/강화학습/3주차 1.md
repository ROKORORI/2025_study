- Markov Property
- MDP (Markov Decision Process)
	- S, A, P, R, r을 정의
		- r = gamma임
		- S: state space
		- A: Action space
		- P: State transition probability from s to s' given action alpha
			- s'은 이전 state를 의미
			
			- P^a_ss' = p(s' | s, a)이다.
		
		- R: Reward function
			- 종류: R_s, R_sa, R_ss'
		- r: Discount factor
			- grid world에서 107번 움직여서 1점을 얻은것과 7번 움직여서 1점을 얻은것이 같은 점수인것을 방지
	
	- ppt 15
		- state
			- s0, s1, s2
		- action
			- a1, a0
		- reward
			- s1 -> a0 -> s0 = 5
				- R(s1, a0, s0) = 5
			- s2 -> a1 -> s0 = -1
				- R(s2, a1, s0) = -1
		- P^a1_s0s1 = s0에서 a1을 선택했을때, s1에 갈확률 = 0
			- P(St+1 = S1 | St = s0, a1) 이라고도 씀
		- R^a1_s1s2 = s1에서 a1을 선택했을때, s2가 받을 reward
	
	- Environment model in MDP
		- Model-based
			- Known MDP
		- Model-free
			- Unknown MDP
		
		- MDP theory는 S, A가 finite해도 됨
	
- Grid world 예제
	- State set = {(1,1), (1,2), (1, 3), (1,4), (2,1), (2, 3), (2, 4), (3, 1), (3, 2), (3, 3), (3, 4)}
	- Action set = {(0, 1), (0, -1), (1, 0), (-1, 0)}
	- State transition probability
		- 1) P^north_(3,1)(3,2) = 설정해줌, 사전의 정의가능
	- Reward
		- 1) R^east_(3, 3)(4,3) = 1 \* east 갈 확률
		- 2) R^north_(3, 2)(4,2) = -1 \*  north로 간후, reward 받을 확률
		- 3) R^alpha_SS' = c, c = small negative value를 가지도록함
			- policy 설정
		
	- Episode
		- Reward를 얻기까지의 과정
	
	- Optimal Policy
		- pi\* = 내가 설계한 모델에 대해 최적의 행동을 하는 것
			- pi\* = S -> A