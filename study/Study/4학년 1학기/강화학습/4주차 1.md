- Notation
	- 같은 물리량을 나타냄
		- Reward
			- 보상
			- 환경에서 즉각적인 피드백
		- Return
			- episode시 나오는 total reward
		- Value Function
			- return을 받을 수 있는 예측값
	
	- action value function
	- state value function

- 강의에서 (a, b)일 때, a = column, b = row를 의미 
- quiz4
	- 1)
		- 풀이과정
			- reward sum
			- 0.8 \* -1 \* -0.04 + 0.1 \* -0.04 + 0.1 \* -0.04
			- -0.84
	- 2)
		- 행동이 결정됨, -1 - 0.04 = -1.04
		
	- 3)
		- 80확률로 제자리, 10퍼 왼, 10퍼 오른
		- 0.8 * -0.04 + 0.1 * -0.04 + 0.1 \* (1 - 0.04)
- quiz5
	- 각기 다른 episode에서 return 값
	- 1)
		- G0 = -0.04 + -0.04 ^ 0.9 + 0.9 ^ 2 * -0.04 + 0.9^3 * -0.04 + 0.9^4 * (-1 - 0.04) = -0.8
	- 2)
		- G0 = -0.04 + -0.04 * 0.9 + -0.04 * 0.9^2 + 0.9^3 * -0.04 + 0.9^4 * (1 - 0.04) = 0.5

- value function
	- policy 입장에선 좋은 return을 만드는게 좋다
	
	- value function은 each state의 goodness를 측정함
	- policy pi가 따라가서 expectation of returns Gt(return)이 최대화 되도록 함

- state value function
	- state의 value가 얼마인지를 나타냄 (expected return)
	- v_pi(s) = e_pi\[Gt | St = s]
- action value function
	- state에서 action값을 선택을 했을 때, 받을 수 있는 expected reuturn

- action value function을 summation하면 state value function이 됨
- advantage function(s, a)
	- action value function - state value function